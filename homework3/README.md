The main portion of code I wrote is in `spark-heart-disease.py` and `smoke-data-imputation-web-scraping.py`. The data from web scraping is saved in `age_smoke_cdc.csv`, `gender_smoke_cdc.csv`, and `smoke_dict_abs.csv`. The report generated by the model is in `heart_disease_report.txt`.

## Task 5: Select your final model based on relevant criteria.

#### Model: Random Forest

Accuracy: 0.722

Precision: 0.723

Recall: 0.708

F1: 0.748


#### Model: Logistic Regression

Accuracy: 0.722

Precision: 0.722

Recall: 0.722

F1: 0.722


#### Model: Decision Tree

Accuracy: 0.708

Precision: 0.710

Recall: 0.694

F1: 0.651


#### Model: Gradient Boosting

Accuracy: 0.722

Precision: 0.722

Recall: 0.681

F1: 0.693


Much like in my Homework 2, I think Random Forest is the best choice here. The Random Forest has the highest accuracy, precision, recall, and F1 score, which are all very important criteria in choosing an accurate predictor.

## How to run my code locally:

    docker pull bitnami/spark:latest
In Windows Powershell, run the following command (or run an equivalent command for your terminal):

    docker run -it --rm -v "${PWD}:/app" -w /app bitnami/spark:latest /bin/bash
Then in docker, run:

    pip install pyspark numpy pandas

    cd spark-on-AWS

    spark-submit spark-heart-disease.py


## How to see my code on AWS EMR:

The ERM cluster that ran my final code is named "erm_cluster_ryannewkirk_3" and the step that ran my spark is "s-02767932WFFUE2SR1AX9".

My S3 bucket is located in is at "s3://de300spring2024/ryan_newkirk"
